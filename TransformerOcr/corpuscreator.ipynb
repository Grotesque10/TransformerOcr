{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84024af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from 'C:/Users/verno/OneDrive/Documents/Desktop/ocrresearch/data6/words.txt'...\n",
      "✅ Successfully created 'corpus2.txt' with 10479 lines.\n",
      "You can now proceed to build your KenLM model with this corpus file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the input and output filenames\n",
    "source_filename = \"iam/words.txt\"\n",
    "corpus_filename = \"corpus.txt\"\n",
    "\n",
    "# This dictionary will hold the sentences, with the line ID as the key\n",
    "lines = defaultdict(list)\n",
    "print(f\"Reading from '{source_filename}'...\")\n",
    "\n",
    "try:\n",
    "    # Open and read the source file\n",
    "    with open(source_filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            # Skip comment lines\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            # Split the line into parts and handle potential errors\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 9:\n",
    "                continue # Skip malformed lines\n",
    "\n",
    "            # The word ID is the first part, e.g., 'a01-000u-00-00'\n",
    "            word_id = parts[0]\n",
    "            \n",
    "            # The actual transcribed word is the last part\n",
    "            transcription = parts[-1]\n",
    "            \n",
    "            # Create a line ID by taking the first three parts of the word ID\n",
    "            line_id = \"-\".join(word_id.split(\"-\")[:3])\n",
    "            \n",
    "            # Append the word to the corresponding sentence list\n",
    "            lines[line_id].append(transcription)\n",
    "\n",
    "    # Now, write the collected sentences to corpus.txt\n",
    "    with open(corpus_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line_id in sorted(lines.keys()):\n",
    "            # Join the words to form a full sentence and write it to the file\n",
    "            sentence = \" \".join(lines[line_id])\n",
    "            f.write(sentence + \"\\n\")\n",
    "            \n",
    "    print(f\"✅ Successfully created '{corpus_filename}' with {len(lines)} lines.\")\n",
    "    print(\"You can now proceed to build your KenLM model with this corpus file.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: The file '{source_filename}' was not found.\")\n",
    "    print(\"Please make sure it is in the same directory as your script.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3593c3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
